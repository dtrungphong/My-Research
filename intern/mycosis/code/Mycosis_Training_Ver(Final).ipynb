{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC363xGUlU4W"
   },
   "outputs": [],
   "source": [
    "# MYCOSES PROJECT: THE CLASSIFIER FOR SKIN FUNGAL DISEASES\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# IMPORT NECCESSARY PACKAGES\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import math #mark\n",
    "\n",
    "import datetime #timeline\n",
    "\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from keras.optimizers import  SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile, rmtree\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# 0. HYPERPARAMETERS\n",
    "\n",
    "# cross-validation split rate -- used in part 5-b\n",
    "Split_Rate=0.1\n",
    "\n",
    "# augmentation arguments -- 6-a\n",
    "Rescale=1./255\n",
    "Rotation_Range=40\n",
    "Width_Shift_Range=0.2\n",
    "Height_Shift_Range=0.2\n",
    "Shear_Range=0.2\n",
    "Zoom_Range=0.2\n",
    "Fill_Mode='nearest'\n",
    "\n",
    "# classification, access methods -- 6-b,c\n",
    "Batch_Size =100\n",
    "Class_Mode='categorical'\n",
    "Target_Size=(150,150)\n",
    "\n",
    "# pretrained model -- 1-a,b -- now use Inception, could change in part 1\n",
    "Weight_File='inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "Last_Layer = 'mixed7'\n",
    "\n",
    "# top layers -- 7-a\n",
    "Dropout_Final=0.2\n",
    "\n",
    "\n",
    "# running -- 8\n",
    "Max_Acc= 0.999\n",
    "Epochs=1\n",
    "# Steps_Train : computed later in part 5\n",
    "# Steps_Test\n",
    "\n",
    "#compiler -- 7-c\n",
    "Lr=0.01\n",
    "Loss='categorical_crossentropy'\n",
    "Metrics='acc'\n",
    "Decay=0.0\n",
    "Momentum=0.0\n",
    "Optimizer= optimizers.SGD(lr=Lr, momentum=Momentum,\n",
    "                                decay=Decay, nesterov=False,\n",
    "                                clipnorm=1, clipvalue=0.5)\n",
    "\n",
    "#data structure\n",
    "\n",
    "To_Group={'tinea': ['Tinea_capitis','Tinea_pedis','Tinea_unguim','Body_Tinea'],\n",
    "            'candidasis': ['Balanitis', 'Candida_onychomycosis','Oral_candidiasis',\n",
    "                           'Skin_candidiasis', 'Vulvovaginal_candidiasis']}\n",
    "To_Delete={}\n",
    "If_Retain=False\n",
    "To_Retain ={}\n",
    "\n",
    "# To reference for optimizer choices:\n",
    "\n",
    "# SGD: support momentum, learning rate decay, Nesterov momentum.\n",
    "# keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False, clipnorm=1, clipvalue=0.5)\n",
    "\n",
    "# RMSprop: usually a good choice for recurrent neural networks\n",
    "# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "# Adagrad: how frequently a parameter gets updated, more updates, smaller lr\n",
    "# keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "\n",
    "# Adadelta: more robust Adagrad, adapts lr based on a moving window of gr updates,\n",
    "# not accumulating all past gradients, continues learning even had many updates\n",
    "# keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "# Adam:amsgrad: if apply the AMSGrad variant from the paper 'On the Convergence'\n",
    "# keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Adamax: a variant of Adam based on the infinity norm\n",
    "# keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "# Nadam: Nesterov Adam optimizer, like Adam is essentially RMSprop with momentum,\n",
    "# Nadam is Adam RMSprop with Nesterov momentum.\n",
    "# keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "p2c2WQ_YrL9j",
    "outputId": "52884e1a-b526-45ea-9cac-f11e3ab263c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-09-12 15:35:29--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 87910968 (84M) [application/x-hdf]\n",
      "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
      "\n",
      "/tmp/inception_v3_w 100%[===================>]  83.84M   126MB/s    in 0.7s    \n",
      "\n",
      "2019-09-12 15:35:30 (126 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 1. CHOOSE PRETRAINED MODEL\n",
    "\n",
    "# a. load weight\n",
    "LOCAL_PATH='/tmp/'\n",
    "weight_file=Weight_File\n",
    "LOCAL_WEIGHT_PATH= os.path.join (LOCAL_PATH, weight_file)\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# b. import model\n",
    "# - import model (Inception3, Resnet50, ..)\n",
    "# - freeze some layers\n",
    "# - get output of the last layer (input of layers built in 7)\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "pretrained_model = InceptionV3(input_shape=(150,150,3), include_top=False, weights=None)\n",
    "pretrained_model.load_weights(LOCAL_WEIGHT_PATH)\n",
    "for layer in pretrained_model.layers:\n",
    "  layer.trainable=False\n",
    "# for testing only: pretrained_model.summary()\n",
    "last_layer=pretrained_model.get_layer(Last_Layer)\n",
    "last_output=last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgWrUpi-xrkb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s49J44S0vLEo"
   },
   "outputs": [],
   "source": [
    "# 2. UTILITY FUNCIONS\n",
    "\n",
    "# a. functions to encapsulate try, except, pass:\n",
    "#    safer choices than os.mkdir(), os.remove(), ..\n",
    "\n",
    "def exist (path):\n",
    "  return (os.path.isdir(path) or os.path.isfile(path))\n",
    "\n",
    "def checkmk(path):\n",
    "  try:\n",
    "    os.mkdir(path)\n",
    "  except:\n",
    "    pass\n",
    "def checkremove(path):\n",
    "  try:\n",
    "    os.remove(path)\n",
    "  except:\n",
    "    pass\n",
    "def checkunlink(path):\n",
    "  try:\n",
    "    os.unlink(path)\n",
    "  except:\n",
    "    pass\n",
    "def checkrm(path):\n",
    "  try:\n",
    "    os.rmdir(path)\n",
    "  except:\n",
    "    pass\n",
    "def checkrmtree(path):\n",
    "  try:\n",
    "    rmtree(path)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# b. functions to encapsulate erase, create:\n",
    "#   safer choices to have folders with completely empty content\n",
    "\n",
    "def reinit(path):                  # start from scratch a folder\n",
    "  checkrmtree(path)\n",
    "  checkmk(path)\n",
    "\n",
    "def clearcontent(path):            # start from scratch the content of a folder\n",
    "  toRemove = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "  for f in toRemove:\n",
    "    try:\n",
    "      if os.path.isfile(f): checkunlink(f)\n",
    "      elif os.path.isdir(f): checkrmtree(f)\n",
    "    except Exception as e:\n",
    "      print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhLdX8ZnaKoD"
   },
   "outputs": [],
   "source": [
    "# c. functions moving files : to add or to replace? to copy or to move?\n",
    "#    move a set of files, a folder's content, or group folders?\n",
    "\n",
    "# keep_src, clear_des=True, False-> copy some files of src to des\n",
    "# True, True-> copy some files of src to replace all files of des\n",
    "# False, False-> move some files of src to des\n",
    "# False, True-> move some files of src to replace all files of des\n",
    "def move_files(files, source, destination, keep_src=True, clear_des=True):\n",
    "  if clear_des: clearcontent(destination)\n",
    "  for filename in files:\n",
    "    from_path=os.path.join(source,filename)\n",
    "    to_path=os.path.join(destination,filename)\n",
    "    if os.path.getsize(from_path) > 0:\n",
    "      copyfile(from_path, to_path)\n",
    "    else:\n",
    "      print(image + \" is zero length, so ignoring.\")\n",
    "    if (not keep_src): checkremove(from_path)\n",
    "\n",
    "# keep_src, clear_des=True, False-> copy content(all files) of src to des\n",
    "# True, True-> copy content of src to replace content of des.\n",
    "# False, False-> move content of src to des\n",
    "# False, True-> move content of src to replace content of des\n",
    "def move_dir_content(source, destination, keep_src=True, clear_des=False):\n",
    "  files=os.listdir(source)\n",
    "  move_files(files, source, destination, keep_src, clear_des)\n",
    "\n",
    "# move content of src to des then delete src.\n",
    "def group (source, destination):\n",
    "  move_dir_content(source, destination, keep_src=False)\n",
    "  checkrm(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "jDUB3wBwr0nX",
    "outputId": "960d2a55-90f9-433b-de57-3ae0d10ddb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drivefs_ipc.0_shell', 'drivefs_ipc.0', 'mycoses_dataset.zip', 'mycoses_dataset']\n",
      "['drivefs_ipc.0_shell', 'drivefs_ipc.0', 'mycoses_dataset']\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n",
      "Hey, there exists no such file or directory!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. LOAD DATA\n",
    "# b. determine base, local, source\n",
    "# - base - original data space: /content\n",
    "# - local - our working space : /tmp\n",
    "# - source - data in our working space: temp/mycoses_dataset\n",
    "\n",
    "BASE_PATH = './TLU-AI/MYCOSES_DATASET/'\n",
    "zip_file_name='mycoses_dataset.zip'\n",
    "BASE_ZIP_PATH = os.path.join(BASE_PATH, zip_file_name)\n",
    "LOCAL_ZIP_PATH= os.path.join(LOCAL_PATH, zip_file_name)\n",
    "source='mycoses_dataset'\n",
    "SOURCE_PATH=os.path.join(LOCAL_PATH, source)\n",
    "\n",
    "# c. copy from base to local\n",
    "# - clear local\n",
    "# - copy\n",
    "clearcontent(LOCAL_PATH)\n",
    "copyfile (BASE_ZIP_PATH, LOCAL_ZIP_PATH)\n",
    "\n",
    "# d. unzip file in local (auto-create source folder) then delete .zip\n",
    "zip_ref=zipfile.ZipFile(LOCAL_ZIP_PATH,'r')\n",
    "zip_ref.extractall(LOCAL_PATH)\n",
    "zip_ref.close()\n",
    "print (os.listdir(LOCAL_PATH))\n",
    "checkremove(LOCAL_ZIP_PATH)\n",
    "print (os.listdir(LOCAL_PATH))\n",
    "\n",
    "# 4.RESTRUCTURE DATA\n",
    "\n",
    "# a. group folders\n",
    "#   to group some diseases into one large class of diseases.\n",
    "#   this grouping could be freely modified later.\n",
    "#   recall that:\n",
    "#   To_Group={'tinea': ['Tinea_capitis','Tinea_pedis','Tinea_unguim','Body_Tinea'],\n",
    "#             'candidasis': ['Balanitis', 'Candida_onychomycosis','Oral_candidiasis',\n",
    "#                            'Skin_candidiasis', 'Vulvovaginal_candidiasis']}\n",
    "for each_class in To_Group:\n",
    "  to_path=os.path.join(SOURCE_PATH, each_class)\n",
    "  reinit (to_path)\n",
    "  for member in To_Group[each_class]:\n",
    "    from_path=os.path.join(SOURCE_PATH,member)\n",
    "    if exist(from_path):\n",
    "      group(from_path, to_path)\n",
    "    else:\n",
    "      print ('Hey, there exists no such file or directory!')\n",
    "\n",
    "# b. delete folders\n",
    "#    to delete some diseases (too rare, create unbalance, ..)\n",
    "#    this deletion could be freely modified later.\n",
    "#    recall that:\n",
    "#    To_Delete={}\n",
    "for each_class in To_Delete:\n",
    "  checkrmtree (os.path.join(SOURCE_PATH, each_class))\n",
    "\n",
    "classes = [folder for folder in os.listdir(SOURCE_PATH)\n",
    "           if os.path.isdir(SOURCE_PATH + '/' + folder)\n",
    "           and len(os.listdir(SOURCE_PATH + '/' + folder))!=0]\n",
    "num_class=len(classes) # the number of classes to be classified\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pocyEsazHTlJ",
    "outputId": "f6893ec9-fc21-4363-9225-5a1d9f3a3a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 0, 'testing': 0} {'training': 0, 'testing': 0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. PREPROCESS\n",
    "\n",
    "# a. make training folder, testing folder\n",
    "stages ={'training': None,'testing':None}\n",
    "stage_folders = {s: os.path.join(LOCAL_PATH, s) for s in stages}\n",
    "for sf in stage_folders:\n",
    "  reinit (stage_folders[sf])\n",
    "\n",
    "# b. split training set, testing set\n",
    "sums={s:0 for s in stages}\n",
    "for c in classes:\n",
    "  SOURCE_CLASS_PATH = os.path.join(SOURCE_PATH, c)\n",
    "  data_set=os.listdir(SOURCE_CLASS_PATH)\n",
    "  stages['training'], stages['testing'] = train_test_split(data_set, test_size=Split_Rate)\n",
    "  class_folders={ s: os.path.join(stage_folders[s], c) for s in stages}\n",
    "  for s in stages:\n",
    "    reinit (class_folders[s])\n",
    "    move_files(stages[s],SOURCE_CLASS_PATH,class_folders[s])\n",
    "    sums[s]+=len(os.listdir(class_folders[s])) #mark\n",
    "steps={s: math.ceil(sums[s]/Batch_Size) for s in stages}#mark\n",
    "print(sums, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Eil0cWyjzUyQ",
    "outputId": "631c736f-5134-4188-fe80-f62f4c0671b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "#6. GENERATE DATA\n",
    "# a. augmentate and normalize\n",
    "# b. determine access method (batch_size, target_size, ..)\n",
    "\n",
    "stage_gen={s: ImageDataGenerator(rescale=Rescale,\n",
    "                             rotation_range=Rotation_Range,\n",
    "                             width_shift_range=Width_Shift_Range,\n",
    "                             height_shift_range=Height_Shift_Range,\n",
    "                             shear_range=Shear_Range,\n",
    "                             zoom_range=Zoom_Range,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             fill_mode=Fill_Mode)\n",
    "              for s in stages }\n",
    "\n",
    "\n",
    "stage_generators={s: stage_gen[s].flow_from_directory(stage_folders[s],\n",
    "                                              batch_size =Batch_Size,\n",
    "                                              class_mode=Class_Mode,\n",
    "                                              target_size=Target_Size)\n",
    "                      for s in stages }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "plQ28tyJzjf6",
    "outputId": "535883f7-6e56-4013-8064-4d0511216497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 7. COMPLETE MODEL\n",
    "\n",
    "# a. add our own top layers\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "# - Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "x = layers.Dense(1024,activation='relu')(x)\n",
    "# - Add a dropout rate of 0.2\n",
    "x = layers.Dropout(Dropout_Final)(x)\n",
    "# - Add a final sigmoid layer for classification\n",
    "x = layers.Dense(num_class,activation='softmax')(x)\n",
    "print (num_class)\n",
    "\n",
    "# b. combine the pretrained-model with our layers\n",
    "model=Model(pretrained_model.input,x)\n",
    "\n",
    "\n",
    "# c. choose optimization method, loss function, and criteria to observe\n",
    "model.compile(optimizer=Optimizer, loss=Loss, metrics=[Metrics])\n",
    "\n",
    "#for testing only: model.summary()\n",
    "\n",
    "# d. build callback\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if (logs.get('acc')>0.999):\n",
    "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
    "      self.stop_training=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "-XdZRiVrM6ED",
    "outputId": "1222c6e5-951a-4c71-daf0-0779ac81b582"
   },
   "outputs": [],
   "source": [
    "# 8. TRAINING\n",
    "time_run1 = datetime.datetime.now() #note\n",
    "callbacks = myCallback()\n",
    "history = model.fit_generator(\n",
    "            stage_generators['training'],\n",
    "            validation_data = stage_generators['testing'],\n",
    "            steps_per_epoch = steps['training'],\n",
    "            epochs = Epochs,\n",
    "            validation_steps = steps['testing'],\n",
    "            verbose = 2,\n",
    "            callbacks=[callbacks]) #mark\n",
    "time_run2 = datetime.datetime.now() #note\n",
    "print(time_run2-time_run1) #note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "MY16rnMfNIi7",
    "outputId": "f8f62610-600e-41b9-dc79-85e6134ac8b4"
   },
   "outputs": [],
   "source": [
    "# 9. PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc='right')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc='right')\n",
    "plt.show()\n",
    "# Desired output. Charts with training and validation metrics. No crash :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yik8JQI_Xitz"
   },
   "outputs": [],
   "source": [
    "# 10. CONFUSION MATRIX, CLASSIFICATION REPORT\n",
    "# add confusion matrix and classification report\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(stage_generators['testing'])\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(stage_generators['testing'].classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = classes\n",
    "print(classification_report(stage_generators['testing'].classes, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
